%\appendix
\chapter{Bench for HPC}

\section{Introduction}
% ==================== de

In this section aims at providing basic but reliable guidlines to produce fast and mannagable code
for our algorithms

Most compilers with which you are probably familiar are standalone programs which take as input some source code text and compile it into machine code (or some other target representation).

cach miss : une donnée n'est pas dans le cache

\cite{Drepper2007}
\cite{Akanksha2012}

%Terminal : sysctl -a | grep machdep.cpu
%
%Terminal : system_profiler SPHardwareDataType
%
%Intel(R) Core(TM) i7-2640M CPU 2.80GHz

L1 = 64kB
L2 = 512kB
L3 = 4096kB

\section{Languages}
% =================
- Csharp
- Julia
- C++
- Intel MKL
- OpenBLAS

\section{From syntax to processor}
% ================================

A short story about how a code is translated to get machin instructions

\newpage
\section{Memory allocation and garbage collection}
% =====================================

Toutes les syntaxes ne sont pas égales en terme de gestion de la mémoire.
Le problème, c'est le passage de la GC qui est couteux en temps. Donc il faut essayer de minimiser l'utilisation mémoire. Idéalement le problème peut rester dans le cache du processeur (mémoire d'accès bcp plus rapide que la RAM). Donc la meilleur stratégie consiste à pré-allouer les tableaux et à faire des opération "in-place" au maximum, c'est à dire d'écraser les donner au fur et à mesure du calcul.

Par ailleurs les accès mémoires sont lents. 
Plus la taille du problème reste petite, plus le problème peut être résolu en restant dans le cache. (latency).

http://stackoverflow.com/questions/4087280/approximate-cost-to-access-various-caches-and-main-memory

MOST CPU’s today uses the memory on multiple level.  Generally  the  memory  at  the  proximity  of  CPU  is costly and less, whereas the memory at the distance (wire distance)  is  bigger,  slower  and  cheap [1].  Today  getting  the  computer  in  market  with  8GB  DRAM  is  cheap,  but  L1/L2  cache  of  such  computer  is  very  small  in  terms  of 10’s  of  KB’s  and  few  MB’s  respectively.  The  access time  of  L1  (that  is  generally  SRAM) is  few  cycles whereas  L2  is  few  10’s  cycles  and  accessing  main memory  is  considered  a  bad  programming  if  accessed too  frequently.  The  access  time  is  huge  and  in  terms  of 100’s of cycles. So optimizing the code to run and access L1  Instruction  and  Data  cache  is  the  simplest  way  to optimizing the code.

\usemintedstyle{github}
\begin{listing}[ht]
\inputminted[
linenos,
breaklines,
frame=lines,
framesep=2mm,
rulecolor=\color{Tdarkgray},
%baselinestretch=1.2,
%bgcolor=Tgray,
fontsize=\footnotesize]
{julia}{ap2_bench/bench/sqrt_memoryusage_code.jl}
\caption{Example from external file}
\label{listing:3}
\end{listing}

On remarque que l'allocation mémoire est très différente d'une fonction à l'autre. Il est important de privilégier des opération "in-place" pour contenir l'allocation mémoire, sinon on risque de déclencher la GC qui est couteuse.

\begin{table}[h]
\centering
\pgfplotstabletypeset[
	every head row/.style={before row=\toprule, after row=\midrule}, 
	every last row/.style={after row=\bottomrule},
	col sep=comma,
	columns={SQRT,CPU,ALLOC},
	columns/SQRT/.style={string type, column name = sqrt | Float64, column type = {l}},
	columns/CPU/.style={fixed zerofill,precision=0, column name = CPU (ns/el), column type = {r}},
	columns/ALLOC/.style={fixed zerofill,precision=0, column name = Allocation (Bytes), column type = {r}},
	]{ap2_bench/bench/sqrt_memoryusage_data.csv}
\caption{Memory allocations for various methods computing sqrt(a) for $n = 10^4$}
\end{table}


\newpage


Les temps CPU sont indicatifs car le bench est fait sur une durée caractéristique trop courte


Ici on met en évidence la non linéarité du coût d'allocation par élément d'un tableau de taille n.
On remarque que la différence entre le coût de sqrt et le coût de l'allocation est constante : c'est le coût de sqrt 
hors allocation. Attention, cette notion est "language dependent" car les allocations sont gérées par la GC.

\begin{listing}[]
\inputminted[
linenos,
breaklines,
frame=lines,
framesep=2mm,
rulecolor=\color{Tdarkgray},
%baselinestretch=1.2,
%bgcolor=Tgray,
fontsize=\footnotesize]
{julia}{ap2_bench/bench/sqrt_allocationimpact_code.jl}
\caption{Example from external file}
\label{listing:3}
\end{listing}


Remarque, on trouverait sans doute la même chose pour MKL, à cause du marshalling : le coût d'appel à une fonction C est supérieur à celui d'une fonction managée (cf HPC .Net)

\newpage


\begin{figure}[h]
\begin{center}
\begin{tikzpicture}
	\begin{axis}[
	xmode=log,
	width = 12cm,
	grid=major,
	legend entries={allocation, sqrt, diff}
	]
	
	\pgfplotsset{table/col sep = comma}
 	\pgfplotstableread{ap2_bench/bench/sqrt_allocationimpact_data.csv}\data;
	
	\addplot [black, smooth, thick, dashed]
       	table [x expr=\thisrowno{0}, y expr=\thisrowno{1}] {\data};

	\addplot [black, smooth, thick]
       	table [x expr=\thisrowno{0}, y expr=\thisrowno{2}] {\data};
	
	\addplot [Tblue, smooth, thick]
       	table [x expr=\thisrowno{0}, y expr=\thisrowno{3}] {\data};

\end{axis}
\end{tikzpicture}
\end{center}
\caption{Nonlinear cost of CPU time in ns/el of memory allocation for arrays (Float64).}
\end{figure}

\input{ap2_bench/bench/arithmetic_cpu_conf_chart}
\input{ap2_bench/bench/arithmetic_cpu_rel_chart}
\input{ap2_bench/bench/arithmetic_cpu_type_chart}

\newpage

profiling :
https://software.intel.com/en-us/intel-vtune-amplifier-xe


SIMD :
http://www.drdobbs.com/architecture-and-design/simd-enabled-vector-types-with-c/240168888

https://software.intel.com/en-us/articles/optimize-for-intel-avx-using-intel-math-kernel-librarys-basic-linear-algebra-subprograms-blas-with-dgemm-routine

\begin{itemize}
\item https://msdn.microsoft.com/en-us/library/ms973852.aspx
\item http://www.sebastiansylvan.com/post/why-most-high-level-languages-are-slow/
\item http://creamysoft.blogspot.fr/2013/05/c-vs-c-performance.html
\item http://www.codeproject.com/Articles/212856/Head-to-head-benchmark-Csharp-vs-NET
\item https://software.intel.com/en-us/articles/speeding-up-c-code-with-the-vtune-amplifier-xe-performance-profiler
\item http://jonathankinlay.com/index.php/2015/02/comparison-programming-languages/
\end{itemize}






%{ap2_bench/csv/sqrt_memoryusage.csv}
\bibliographystyle{alpha}
\bibliography{../bibliography}
